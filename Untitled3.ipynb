{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return pd.datetime.strptime('190' + x, '%Y-%m')\n",
    "\n",
    "def parser2(x):\n",
    "    return pd.datetime.strptime(x,'%Y-%m')\n",
    "\n",
    "# create a df to have a supervised learning problem\n",
    "def timeseries_to_supervised(data,lag=1):\n",
    "    df = pd.DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1,lag+1)]\n",
    "    columns.append(df)\n",
    "    df = pd.concat(columns,axis=1)\n",
    "    df.fillna(0,inplace=True)\n",
    "    return df\n",
    "\n",
    "# Transform dataset to stationary. This removes the trends from the data that are dependent on time.\n",
    "# One way of stationarizing a dataset is through data differencing resulting to seeing the changes\n",
    "# to the observations from one timestep to the next.\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval,len(dataset)):\n",
    "        value = dataset[i] - dataset[i-interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "    new_row = [x for x in X] + [value]\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0,-1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "    # LSTM layer expects input to be in a matrix with the dimensions: [samples, time steps, features]\n",
    "    X,y = train[:,0:-1], train[:,-1]\n",
    "    X = X.reshape(X.shape[0],1,X.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    acc = []\n",
    "    for i in range(nb_epoch):\n",
    "        print '{}/{} epoch'.format(i+1,nb_epoch)\n",
    "        history = model.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False, validation_split=0.05)\n",
    "        loss.append(history.history['loss'])\n",
    "        acc.append(history.history['acc'])\n",
    "        val_loss.append(history.history['val_loss'])\n",
    "        model.reset_states()\n",
    "    # summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(loss,label='loss')\n",
    "    plt.plot(val_loss,label='val_loss')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "    X = X.reshape(1,1, len(X))\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    return yhat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input files\n",
    "shampoo = 'shampoo-sales.csv'\n",
    "airplane = 'international-airline-passengers.csv'\n",
    "sp500 = 'sp500.csv'\n",
    "sin = 'sinwave.csv'\n",
    "data = np.arange(1,51,.10)\n",
    "\n",
    "# load dataset\n",
    "# series = pd.Series(data)\n",
    "# series = pd.read_csv(airplane, header=0,\n",
    "                 # parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "series = pd.read_csv(sp500,squeeze=True)\n",
    "# series = pd.read_hdf('cex-data.hdf','cex-1d').closing\n",
    "\n",
    "# 1. transform data to be stationary\n",
    "raw_values = series.values\n",
    "diff_values = difference(raw_values, 1)\n",
    "\n",
    "# 2. transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values\n",
    "\n",
    "# split data into train and test\n",
    "train_size = int(len(supervised_values) * 0.67)\n",
    "test_size = len(supervised_values) - train_size\n",
    "train, test = supervised_values[0:train_size,:], supervised_values[train_size:len(supervised_values),:]\n",
    "\n",
    "# 3. transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1376, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 16s - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.0090 - val_acc: 0.0000e+00\n",
      "2/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 12s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0092 - val_acc: 0.0000e+00\n",
      "3/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 11s - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.0000e+00\n",
      "4/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 11s - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "5/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 10s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "6/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 10s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.0000e+00\n",
      "7/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 9s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "8/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 11s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "9/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 11s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "10/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 11s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "11/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 12s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "12/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 10s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "13/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 11s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "14/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 11s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.0000e+00\n",
      "15/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 10s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.0000e+00\n",
      "16/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 8s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "17/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 10s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.0000e+00\n",
      "18/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 12s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "19/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 13s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "20/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 13s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "21/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 11s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "22/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "23/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "24/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "25/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 7s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "26/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 5s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.0000e+00\n",
      "27/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "28/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "29/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 7s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "30/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "31/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "32/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "33/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 5s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "34/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "35/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 5s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "36/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "37/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 7s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "38/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "39/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 5s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "40/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "41/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "42/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 8s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "43/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 14s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "44/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 15s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "45/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 12s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "46/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "47/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "48/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 9s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "49/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 7s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "50/50 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 7s - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "lstm_model = fit_lstm(train_scaled, 1, 50, 10) # 1 batch, 3000 epoch, 4 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM layer expects input to be in a matrix with the dimensions: [samples, time steps, features]\n",
    "X,y = train_scaled[:,0:-1], train_scaled[:,-1]\n",
    "X = X.reshape(X.shape[0],1,X.shape[1])\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, batch_input_shape=(1, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 9s - loss: 0.0208 - acc: 0.0000e+00 - val_loss: 0.0093 - val_acc: 0.0000e+00\n",
      "2/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 9s - loss: 0.0208 - acc: 0.0000e+00 - val_loss: 0.0091 - val_acc: 0.0000e+00\n",
      "3/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 9s - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "4/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 9s - loss: 0.0208 - acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.0000e+00\n",
      "5/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 7s - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "6/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 5s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0090 - val_acc: 0.0000e+00\n",
      "7/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 6s - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "8/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 5s - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.0090 - val_acc: 0.0000e+00\n",
      "9/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 5s - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.0091 - val_acc: 0.0000e+00\n",
      "10/10 epoch\n",
      "Train on 2653 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "2653/2653 [==============================] - 5s - loss: 0.0206 - acc: 0.0000e+00 - val_loss: 0.0091 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "val_loss = []\n",
    "acc = []\n",
    "nb_epoch = 50\n",
    "for i in range(nb_epoch):\n",
    "    print '{}/{} epoch'.format(i+1,nb_epoch)\n",
    "    history = model.fit(X, y, epochs=1, batch_size=1, verbose=1, shuffle=False, validation_split=0.05)\n",
    "    loss.append(history.history['loss'])\n",
    "    acc.append(history.history['acc'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    model.reset_states()\n",
    "# summarize history for loss\n",
    "plt.figure()\n",
    "plt.plot(loss,label='loss')\n",
    "plt.plot(val_loss,label='val_loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stateful prediction...\n",
      "[[ 0.04645014]] -0.0132716335774 0.00290124\n",
      "RMSE: 0.0161728738294\n"
     ]
    }
   ],
   "source": [
    "# make test input\n",
    "X,y = test_scaled[:,0:-1], test_scaled[:,-1]\n",
    "X = X.reshape(X.shape[0],1,X.shape[1])\n",
    "testX = X\n",
    "testY = y\n",
    "# make predictions\n",
    "predictions = lstm_model.predict(testX,batch_size=1, verbose=0)\n",
    "rmse = sqrt(mean_squared_error([testY[-1]],predictions[-1]))\n",
    "plt.figure(0)\n",
    "print \"Stateful prediction...\"\n",
    "print testX[-1],testY[-1],predictions[-1][0]\n",
    "print \"RMSE: {}\".format(rmse)\n",
    "plt.title('Stateful')\n",
    "plt.plot(testX[:, 0],label='initial')\n",
    "plt.plot(testY,label='shifted true')\n",
    "plt.plot(predictions,label='shifted pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
